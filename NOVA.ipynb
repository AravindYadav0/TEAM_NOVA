{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain transformers huggingface_hub\n"
      ],
      "metadata": {
        "id": "6Fm8_0R-_DJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your_huggingface_token_here\"\n"
      ],
      "metadata": {
        "id": "FbRe_MV7KHJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-base\",\n",
        "    model_kwargs={\"temperature\": 0.6, \"max_length\": 200}\n",
        ")\n",
        "\n",
        "template = \"\"\"\n",
        "You are an AI scheduling assistant for a veterinary clinic.\n",
        "A client canceled their appointment for {service_type} at {time_slot}.\n",
        "Here is the waitlist:\n",
        "\n",
        "{waitlist}\n",
        "\n",
        "Select the best replacement client for this open slot, considering:\n",
        "- Pet urgency or type of service\n",
        "- Client flexibility\n",
        "- Fairness (who has been waiting longest)\n",
        "Explain your reasoning briefly and output the chosen client name.\n",
        "\n",
        "Output format:\n",
        "Client: <name>\n",
        "Reason: <short explanation>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"service_type\", \"time_slot\", \"waitlist\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Example Data\n",
        "service_type = \"vaccination\"\n",
        "time_slot = \"Monday 10:00 AM\"\n",
        "\n",
        "waitlist = \"\"\"\n",
        "1. Sarah - vaccination for puppy, flexible any time\n",
        "2. Tom - surgery follow-up, needs early slot\n",
        "3. Alice - routine check-up, prefers afternoons\n",
        "\"\"\"\n",
        "\n",
        "# Run the AI\n",
        "response = chain.run(service_type=service_type, time_slot=time_slot, waitlist=waitlist)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "aapL4FN8KLRO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}